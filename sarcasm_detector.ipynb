{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A text-based sarcasm detector trained on the reddit sarcasm dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import bcolz\n",
    "import re\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SARC_train_path = 'data/train-balanced.csv'\n",
    "SARC_test_path = 'data/test-balanced.csv'\n",
    "# This refers to the glove embedding size\n",
    "DIM_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_array(fname, arr):\n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpack_glove():\n",
    "    path = 'glove/'\n",
    "    name = '6B.100d'\n",
    "    res_path = 'glove/pickled/'\n",
    "    with open(path+ 'glove.' + name + '.txt', 'r') as f: lines = [line.split() for line in f]\n",
    "    words = [d[0] for d in lines]\n",
    "    vecs = np.stack(np.array(d[1:], dtype=np.float32) for d in lines)\n",
    "    wordidx = {o:i for i,o in enumerate(words)}\n",
    "    save_array(res_path+name+'.dat', vecs)\n",
    "    pickle.dump(words, open(res_path+name+'_words.pkl','wb'))\n",
    "    pickle.dump(wordidx, open(res_path+name+'_idx.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only run this once\n",
    "# unpack_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_glove():\n",
    "    loc = 'glove/pickled/6B.100d'\n",
    "    return (load_array(loc+'.dat'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb')),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_vecs, glove_words, glove_word2id = load_glove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get sarcasm training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use this command in shell to take a peak at the beginning of a large file:\n",
    "# \"head -10 <large file>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tbl = pd.read_csv(SARC_train_path,\n",
    "                       names=[\"label\", \"comment\", \"author\", \"subreddit\", \"score\", \"ups\", \"downs\", \"date\", \"created_utc\", \"parent_comment\"],\n",
    "                       usecols=range(0, 2),\n",
    "                       header=None,\n",
    "                       encoding=\"ISO-8859-1\",\n",
    "                       sep='\\t',\n",
    "                       dtype={\"label\": int, \"comment\": object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0      0                                         NC and NH.\n",
       "1      0  You do know west teams play against west teams...\n",
       "2      0  They were underdogs earlier today, but since G...\n",
       "3      0  This meme isn't funny none of the \"new york ni...\n",
       "4      0                    I could use one of those tools."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tbl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Looks good! But it also looks like it's sorted and I don't want that, so let's shuffle\n",
    "train_tbl = train_tbl.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>But we live in a completely black and white wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Thats to funny!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>But... it's Danny Devito... and you're on Redd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Yeah because that will really take away all th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Yeah this tv phenomenom is ALWAYS over looked.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0      1  But we live in a completely black and white wo...\n",
       "1      1                                    Thats to funny!\n",
       "2      1  But... it's Danny Devito... and you're on Redd...\n",
       "3      1  Yeah because that will really take away all th...\n",
       "4      1     Yeah this tv phenomenom is ALWAYS over looked."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tbl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_lbls = train_tbl.ix[:,0].values\n",
    "train_coms = train_tbl.ix[:,1].values.astype(str).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get sarcasm test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tbl = pd.read_csv(SARC_test_path,\n",
    "                       names=[\"label\", \"comment\", \"author\", \"subreddit\", \"score\", \"ups\", \"downs\", \"date\", \"created_utc\", \"parent_comment\"],\n",
    "                       usecols=range(0, 2),\n",
    "                       header=None,\n",
    "                       encoding=\"ISO-8859-1\",\n",
    "                       sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tbl = test_tbl.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_lbls = test_tbl.ix[:,0].values\n",
    "test_coms = test_tbl.ix[:,1].values.astype(str).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize text and turn it into sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180583\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "comment_length = 200\n",
    "\n",
    "# Tokenize comments, turn them into sequences and pad them\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_coms)\n",
    "train_seqs = tokenizer.texts_to_sequences(train_coms)\n",
    "SARC_word2id = tokenizer.word_index\n",
    "print(len(SARC_word2id))\n",
    "\n",
    "train_seqs = pad_sequences(train_seqs, maxlen=comment_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(test_coms)\n",
    "test_seqs = tokenizer.texts_to_sequences(test_coms)\n",
    "test_seqs = pad_sequences(test_seqs, maxlen=comment_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get positive and negative sentiment words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos = pd.read_csv('data/positive.txt', header=None)\n",
    "neg = pd.read_csv('data/negative.txt', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build embedding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you add the sentiments into the embeddings, it takes a long time.\n",
    "\n",
    "Save it the first time and then load it every time after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build embedding matrix without sentiments\n",
    "embed_mat = np.zeros((len(SARC_word2id) + 1, DIM_SIZE))\n",
    "for word, i in SARC_word2id.items():\n",
    "    # Use .get() instead of [] so it will return None if key\n",
    "    # is not present\n",
    "    embed_id = glove_word2id.get(word)\n",
    "    if embed_id == None:\n",
    "        # If it's not in glove, then use random vector\n",
    "        embed_vec = np.random.normal(0.6, size=(DIM_SIZE,))\n",
    "    else:\n",
    "        embed_vec = glove_vecs[embed_id]\n",
    "        \n",
    "    embed_mat[i,:] = embed_vec\n",
    "\n",
    "embed_mat[-1,:] = np.random.normal(0.6, size=(DIM_SIZE,)) / 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000  embeddings constructed\n",
      "20000  embeddings constructed\n",
      "30000  embeddings constructed\n",
      "40000  embeddings constructed\n",
      "50000  embeddings constructed\n",
      "60000  embeddings constructed\n",
      "70000  embeddings constructed\n",
      "80000  embeddings constructed\n",
      "90000  embeddings constructed\n",
      "100000  embeddings constructed\n",
      "110000  embeddings constructed\n",
      "120000  embeddings constructed\n",
      "130000  embeddings constructed\n",
      "140000  embeddings constructed\n",
      "150000  embeddings constructed\n",
      "160000  embeddings constructed\n",
      "170000  embeddings constructed\n",
      "180000  embeddings constructed\n"
     ]
    }
   ],
   "source": [
    "# Build embedding matrix with sentiments\n",
    "embed_mat = np.zeros((len(SARC_word2id) + 1, DIM_SIZE + 1))\n",
    "for word, i in SARC_word2id.items():\n",
    "    # Use .get() instead of [] so it will return None if key\n",
    "    # is not present\n",
    "    embed_id = glove_word2id.get(word)\n",
    "    embed_vec = np.empty((DIM_SIZE + 1,))\n",
    "    if embed_id == None:\n",
    "        # If it's not in glove, then use random vector\n",
    "        embed_vec[:DIM_SIZE] = np.random.normal(0.6, size=(DIM_SIZE,))\n",
    "    else:\n",
    "        embed_vec[:DIM_SIZE] = glove_vecs[embed_id]\n",
    "    \n",
    "    # Append sentiment signifier\n",
    "    if pos[0].str.contains(word).any():\n",
    "        np.append(embed_vec, 0.5)\n",
    "    elif neg[0].str.contains(word).any():\n",
    "        np.append(embed_vec, -0.5)\n",
    "    else:\n",
    "        np.append(embed_vec, 0.0)\n",
    "        \n",
    "    embed_mat[i,:] = embed_vec\n",
    "    \n",
    "    # Update on progress\n",
    "    if (i % 10000 == 0):\n",
    "        print(i, \" embeddings constructed\")\n",
    "\n",
    "embed_mat[-1,:DIM_SIZE] = np.random.normal(0.6, size=(DIM_SIZE,)) / 3.0\n",
    "embed_mat[-1,-1] = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save embedding matrix so you don't have to re-build it every time\n",
    "# with sentiments\n",
    "outfile = 'data/embed.npy'\n",
    "np.save(outfile, embed_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load embedding matrix with sentiments\n",
    "outfile = 'data/embed.npy'\n",
    "embed_mat = np.load(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout, Dense, Flatten\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers import Embedding, merge\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add one to dim_size if you're including sentiments\n",
    "Embed_Layer = Embedding(len(SARC_word2id)+1,\n",
    "                            DIM_SIZE,\n",
    "                            weights=[embed_mat],\n",
    "                            input_length=comment_length,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keras model\n",
    "model = Sequential([\n",
    "    Embed_Layer,\n",
    "    Convolution1D(128, 3, padding='same', activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    MaxPooling1D(),\n",
    "    Convolution1D(256, 3, padding='same', activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    MaxPooling1D(),\n",
    "    Convolution1D(512, 3, padding='same', activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "#     BatchNormalization(),\n",
    "#     Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(0.01))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1010826 samples, validate on 251608 samples\n",
      "Epoch 1/2\n",
      "1010826/1010826 [==============================] - 570s - loss: 0.6766 - acc: 0.6203 - val_loss: 0.7277 - val_acc: 0.5511\n",
      "Epoch 2/2\n",
      "1010826/1010826 [==============================] - 569s - loss: 0.6671 - acc: 0.6243 - val_loss: 0.7130 - val_acc: 0.5500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9f38f84dd8>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: may have to weight this loss if the classes aren't balanced\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_seqs, train_lbls, validation_data=(test_seqs, test_lbls), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now allow it to train embedding as well\n",
    "model.layers[0].trainable=True\n",
    "model.optimizer.lr=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_seqs, train_lbls, validation_data=(test_seqs, test_lbls), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 100)          18058400  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 200, 128)          64128     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 200, 128)          512       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 100, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100, 128)          512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 50, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 50, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 50, 128)           512       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               640100    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 18,928,761\n",
      "Trainable params: 869,393\n",
      "Non-trainable params: 18,059,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_for_prediction(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    tokenizer.fit_on_texts([text])\n",
    "    text_seqs = tokenizer.texts_to_sequences([text])\n",
    "    text_seqs = pad_sequences(text_seqs, maxlen=comment_length)\n",
    "    \n",
    "    return text_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_sarcastic = \"Isn't getting cheated on great? I just love it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_normal = \"Your dog is beautiful, I'd like to get a labrador some day as well.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.36336055]]\n"
     ]
    }
   ],
   "source": [
    "sample_in = preprocess_for_prediction(sample_sarcastic)\n",
    "prediction = model.predict(sample_in)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.51159787]]\n"
     ]
    }
   ],
   "source": [
    "sample_in = preprocess_for_prediction(sample_normal)\n",
    "prediction = model.predict(sample_in)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = 'model/'\n",
    "model.save_weights(model_path+'SARC_100.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
